<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Mobile3DScanner: An Online 3D Scanner for High-quality Object Reconstruction with a Mobile Device. Published in the Special Issue of TVCG for ISMAR 2021."/>
    <title>Mobile3DScanner: An Online 3D Scanner for High-quality Object Reconstruction with a Mobile Device</title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
    <style>
      body {
        background: #fdfcf9 no-repeat fixed top left;
        font-family:'Open Sans', sans-serif;
      }
    </style>

  </head>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container">
        <div class="row">
          <div class="col">
            <h2 style="font-size:30px;">Mobile3DScanner: An Online 3D Scanner for High-quality Object Reconstruction with a Mobile Device</h2>
            <h5 style="color:#6e6e6e;">Special Issue of IEEE Transactions on Visualization and Computer Graphics for ISMAR 2021</h5>
            <h5 style="color:#ff0000;">ISMAR 2021 Best Journal Paper Nominee</h5>
            <hr>
            <h6> <a target="_blank">Xiaojun Xiang</a><sup>1*</sup>, 
                 <a href="https://sorcererq.github.io/jianghanqing/" target="_blank">Hanqing Jiang</a><sup>1*</sup>, 
                <a href="http://www.cad.zju.edu.cn/home/gfzhang/" target="_blank">Guofeng Zhang</a><sup>2*</sup>,
                <a target="_blank">Yihao Yu</a><sup>1</sup>,
                <a target="_blank">Chenchen Li</a><sup>1</sup>,
                <a target="_blank">Xingbin Yang</a><sup>1</sup>,
                <a target="_blank">Danpeng Chen</a><sup>1</sup>,
                <a href="http://www.cad.zju.edu.cn/home/bao/" target="_blank">Hujun Bao</a><sup>2+</sup></h6>
            <p> <sup>1</sup>SenseTime Research&nbsp;&nbsp; 
                <sup>2</sup>State Key Lab of CAD & CG, Zhejiang University
                <br>
                <sup>*</sup> denotes equal contribution and joint first authorship
                <br>
                <sup>+</sup> denotes corresponding author
            </p>
            <!-- <p> <a class="btn btn-secondary btn-lg" href="" role="button">Paper</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Code</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Data</a> </p> -->

            <div class="row justify-content-center">
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="http://www.cad.zju.edu.cn/home/gfzhang//papers/mobile3dscanner/mobile3dscanner_ismar2021.pdf" role="button"  target="_blank">
                    <i class="fa fa-file"></i> Paper</a> </p>
              </div>
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <img class="img-fluid" src="images/teaser.png" alt="Mobile3DScanner Teaser" width="100%">
        </div>
      </div>
    </div>
  </section>
  <br>
  <br>
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Abstract</h3>
            <hr style="margin-top:0px;">
            <p class="text-justify">
              We present a novel online 3D scanning system for high-quality object reconstruction with a mobile device, called Mobile3DScanner. Using a mobile device equipped with an embedded RGBD camera, our system provides online 3D object reconstruction capability for users to acquire high-quality textured 3D object models. Starting with a simlutaneous pose tracking and TSDF fusion module, our system allows users to scan an object with a mobile device to get a 3D model for real-time preview. After the real-time scanning process is completed, the scanned 3D model is globally optimized and mapped with multi-view textures as an efficient post- process to get the final textured 3D model on the mobile device. Unlike most existing state-of-the-art systems which can only scan homeware objects such as toys with small dimensions due to the limited computation and memory resources of mobile platforms, our system can reconstruct objects with large dimensions such as statues. We propose a novel visual-inertial ICP approach to achieve real-time accurate 6DoF pose tracking of each incoming frame on the front end, while maintaining a keyframe pool on the back end where the keyframe poses are optimized by local BA. Simultaneously, the keyframe depth maps are fused by the optimized poses to a TSDF model in real-time. Especially, we propose a novel adaptive voxel resizing strategy to solve the out-of-memory problem of large dimension TSDF fusion on mobile platforms. In the post-process, the keyframe poses are globally optimized and the keyframe depth maps are optimized and fused to obtain a final object model with more accurate geometry. The experiments with quantitative and qualitative evaluation demonstrate the effectiveness of the proposed 3D scanning system based on a mobile device, which can successfully achieve online high-quality 3D reconstruction of natural objects with larger dimensions for efficient AR content creation.
            </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- Reconstruction Showcase -->
  <br>
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Reconstruction showcase</h3>
            <hr style="margin-top:0px">
            <br>
        </div>
      </div>
    </div>
  </section>
  <section>
    <div class="container">
      <div class="row">
        <div class="col text-center">
            <video width="100%" playsinline="" controls autoplay loop="loop" preload="" muted="">
                <source src="videos/sup-1.mp4" type="video/mp4">
            </video>
        </div>
        <div class="col text-center">
            <video width="100%" playsinline="" controls autoplay loop="loop" preload="" muted="">
                <source src="videos/sup-2.mp4" type="video/mp4">
            </video>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- showcase -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <div class="embed-responsive embed-responsive-16by9">
              <iframe width="640" height="480" src="https://sketchfab.com/playlists/embed?collection=b8cdbcb052f349a9bc6fc2a68e4f2001&autostart=1"
              frameborder="0" allow="autoplay; fullscreen; vr" allowvr=""
              allowfullscreen="" mozallowfullscreen="true" webkitallowfullscreen="true" onmousewheel=""></iframe>            
          </div>
            <br>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- system overview -->
  <br>
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>System overview</h3>
            <hr style="margin-top:0px">

            <img class="img-fluid" src="images/system_overview.png" alt="Mobile3DScanner System Overview" width="80%">
            <br>
            <br>
            <p class="text-justify"> 
              If a user wants to scan a natural object by our system, the object should be put on a horizontal planar surface such as a desk or the ground. 
              As the user scans the object by a mobile device with a rear RGBD camera, our pipeline tracks 6DoF poses of the object in real-time using a visual-inertial ICP (VI-ICP) approach, 
              combined with local BA and loop closure to refine the poses of keyframes. The object is consistently segmented by a spatio-temporal planar surface tracking method. 
              Simultaneously, the incoming depths are fused to a TSDF model for real-time preview, using an adaptive voxel resizing strategy. In the post-process, all keyframe 
              poses and depths are optimized by GBA and SGM, then fused to a TSDF model, followed by Marching Cubes, Poisson Surface Reconstruction (PSR), Shape from Shading (SFS) and 
              Multi-view Texture Mapping to get the final 3D mesh.
            </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- Contributions -->
  <br>
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Contributions</h3>
            <hr style="margin-top:0px">
        </div>
      </div>
    </div>
  </section>
  <br>

  <!--
  <section>
    <div class="container">
      <table width="100%" class="col text-center">
        <tr>
          <h5 class="text-left"><b>Visual-Inertial ICP Tracking</b></h5>
          <br>
        </tr>
        <tr>
          <td width="40%" valign="top">
            <img class="img-fluid" src="images/vi-icp.png" width="100%">
            <p class="text-justify" style="color:#8899a5; font-size:13px"> 
            ICP tracking results on case "David". (a) Original ICP in <a href="http://www.open3d.org/" target="_blank">Open3D</a>. (b) Our VI-ICP approach. (c) (d) VI-ICP combined with LBA and Loop Closure respectively.
            </p>
          </td>
          <td width="5%"></td>
          <td valign="top">
            <img class="img-fluid" src="images/vi-icp-energy.png" width="35%">
              <br>
              <img class="img-fluid" src="images/imu_energy.png" width="35%">
              <br><br>
              <p class="text-justify">
              Our system localizes the camera by loosely coupled integration of ICP and IMU. The IMU state is initialized and optimized with the ICP tracking result and will provide a pose prediction for current frame.
              The rotation part <img class="img-fluid" src="images/rotation.png" width="4%"> of the predicted pose and gravity <img class="img-fluid" src="images/gravity.png" width="4%"> will be integrated into our ICP energy term to enhance the tracking robustness.
              </p>
          </td>
        </tr>
      </table>
    </div>
  </section>
  -->
  <!--vi-icp-->
  <section>
    <div class="container">
      <div class="row">
        <div class="col text-center">
          <h5 class="text-left"><li><b>Visual-Inertial ICP Tracking</b></li></h5>
          <br>
          <img class="img-fluid" src="images/vi-icp.png" width="45%">
          <p class="text-justify; text-center" style="color:#8899a5; font-size:12px"> 
          ICP tracking results on case "David". (a) Original ICP in <a href="http://www.open3d.org/" target="_blank">Open3D</a>. (b) Our VI-ICP approach. (c) (d) VI-ICP combined with LBA and Loop Closure respectively.
          </p>
          <img class="img-fluid" src="images/vi-icp-energy.png" width="20%">
          <br>
          <img class="img-fluid" src="images/imu_energy.png" width="20%">
          <p class="text-justify">
          Our system localizes the camera by loosely coupled integration of ICP and IMU. The IMU module is initialized and optimized with the ICP tracking result continuously and will provide a pose prediction for current frame.
          The rotation part <img class="img-fluid" src="images/rotation.png" width="2%"> of the predicted pose and gravity <img class="img-fluid" src="images/gravity.png" width="2%"> will be integrated into our ICP energy term to enhance the tracking robustness.
          </p>
       </div>
      </div>
    </div>
  </section>
  
  <!--voxel resizing-->
  <br><br>
  <section>
    <div class="container">
      <div class="row">
        <div class="col text-center">
          <h5 class="text-left"><li><b>Adaptive Voxel Resizing</b></li></h5>
          <br>
          <img class="img-fluid" src="images/voxel_resizing.png" width="50%">
          <p class="text-justify;text-center" style="color:#8899a5; font-size:12px"> 
          Adaptive voxel resizing on case “Worker”. (a) Origin voxel size: 4mm. (b) After voxel resizing: 6mm. (c) Final 3D mesh: 4mm.
          </p>
          <p class="text-justify"> 
          On a mobile device, the memory usage of TSDF should be kept under an upper limitation (e.g. 200MB) to avoid OOM. The adaptive voxel resizing strategy enable users to scan as large as possible objects with proper voxel resolution, and  
          will be triggered whenever the memory cost of current TSDF model exceeds the memory limitation. We create a new TSDF volume to represent the object with a larger voxel size (e.g. 1.5 times of origin voxel size) and the new TSDF value is efficiently calculated 
          by trilinear interpolation of the old voxels. For case "Worker", it costs 65.51 ms on iPad Pro 2020 to complete the voxel resizing.
          </p>
        </div>
      </div>
    </div>
  </section>

  <!--MVS with depth prior-->
  <br><br>
  <section>
    <div class="container">
      <div class="row">
        <div class="col text-center">
          <h5 class="text-left"><li><b>Multi-View Stereo with Depth Prior</b></li></h5>
          <br>
          <img class="img-fluid" src="images/sgm.png" width="50%">
          <p class="text-justify; text-center" style="color:#8899a5; font-size:12px"> 
          Depth refinement on case “David”. (a) A keyframe and its two reference ones. (b) original depth map from iPad Pro. (c) SGM without depth prior. (d) SGM with depth prior.
          </p>
          <p class="text-justify">
          The input depths from consumer RGBD camera such as dToF on iPad Pro might have depth errors or over-smoothness with lost geometric details. 
          We propose to estimate more accurate depths with geometric details by SGM. The dToF depth measurements are used as depth priors to compute weights for multi-frame cost function as follows:
          </p>
          <img class="img-fluid" src="images/sgm_cost.png" width="28%">
          <br>
          <p class="text-justify">
          where <img class="img-fluid" src="images/gaussian.png" width="5%"> is the cost fusion weight using a Gaussian function, <img class="img-fluid" src="images/depth_prior.png" width="4%"> is the 
          dToF depth prior.
          </p>
        </div>
      </div>
    </div>
  </section>

  <!--sfs-->
  <br><br>
  <section>
    <div class="container">
      <div class="row">
        <div class="col text-center">
          <h5 class="text-left"><li><b>Efficient Shape-From-Shading</b></li></h5>
          <br>
          <img class="img-fluid" src="images/sfs.png" width="50%">
          <p class="text-justify;text-center" style="color:#8899a5; font-size:12px"> 
          Shape from shading results on case of “David” and "Lion". (a) The 3D models by PSR . (b) The 3D models with SFS.
          </p>
          <p class="text-justify">
          Most existing implementations of SFS are far from feasible to mobile platform due to the limited computation and memory resources. 
          We perform SFS directly on the mesh, by optimizing an intermediate triangle normal map, followed by updating the vertex positions 
          according to the optimization of the normal map. It takes only 3.24 seconds on iPad Pro 2020 CPU with RMSE 3.12mm and MAE 2.473mm,  
          while <a href="https://github.com/NVlabs/intrinsic3d" target="_blank">Intrinsic3D</a> costs 81 minutes on an Intel Core i7 7700K CPU with 32GB RAM and acquire a slightly better result with RMSE 2.89mm and 
          MAE 2.16mm.
          </p>
        </div>
      </div>
    </div>
  </section>

  <!-- Comparison with state-of-the-art methods -->
  <br><br>
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Comparison with state-of-the-art methods</h3>
            <hr style="margin-top:0px">

            <img class="img-fluid" src="images/compare_SOTA.png" alt="Quantitative Comparison" width="90%">
            <p class="text-justify"> 
            Comparison of our Mobile3DScanner with other state-of-the-art methods: (a) Three representative keyframes in case “La Marseillaise”. 
            (b) <a href="http://www.open3d.org/" target="_blank">Open3D</a>. 
            (c) <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ismar2011.pdf" target="_blank">KinectFusion</a>. 
            (d) <a href="https://www.robots.ox.ac.uk/~victor/infinitam/" target="_blank">InfiniTAM</a>. 
            (e) <a href="http://graphics.stanford.edu/projects/bundlefusion/" target="_blank">BundleFusion</a>.
            (f) <a href="https://www.3dscannerapp.com/" target="_blank">3D Scanner App</a>. 
            (g) Ours Mobile3DScanner. 
            (h) The GT model aqcuired by a commercial 3D scanner.
            </p>

            <img class="img-fluid" src="images/model_accuracy.png" alt="Qualitative Comparison" width="90%">
            <p class="text-justify"> 
            The RMSEs and MAEs of the reconstruction results by our Mobile3DScanner and other SOTA methods on our four experimental cases captured by iPad Pro 2020, with each object scanned by a commercial 3D scanner as GT.
            </p>

            <img class="img-fluid" src="images/time_consumption.png" alt="Time Consumption" width="90%">
            <p class="text-justify">
            The detailed time consumptions of our Mobile3DScanner in all the substeps of three cases “Deer”, “La Marseillaise” and “Worker” on an iPad Pro 2020, which contain 487 frames, 1098 frames, and 1438 frames respectively.
            </p>
        </div>
      </div>
    </div>
  </section>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <img class="img-fluid" src="images/more_result.png" alt="Quantitative Comparison" width="100%">
        </div>
      </div>
    </div>
  </section>
  <br>

  
  <!-- citing -->
  <br>
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Citation</h3>
          <hr style="margin-top:0px">
              <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>@article{xiang2021mobile3dscanner,
  title={{Mobile3DScanner}: An Online {3D} Scanner for High-quality Object Reconstruction with a Mobile Device},
  author={Xiang, Xiaojun and Jiang, Hanqing and Zhang, Guofeng and Yu, Yihao and Li, Chenchen and Yang, Xingbin and Chen, Danpeng and Bao, Hujun},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  year={2021}
}</code></pre>
      </div>
    </div>
  </div>

  <!-- ack -->
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Acknowledgements</h3>
          <hr style="margin-top:0px">
          <p class="text-justify">
          We would like to thank Nan Wang, Chongshan Sheng, Li Zhou, Jianjun Xia, Jusong Zhou, and Di Zhang for their kind helps in the development of the Mobile3DScanner system.
          This work was partially supported by NSF of China (Nos. 61822310 and 61932003).
          </p>
      </div>
    </div>
  </div>

  <!-- rec -->
  <!-- 
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Recommendations to other works from our group</h3>
          <hr style="margin-top:0px">
          <p class="text-justify">
            Welcome to checkout our work on Transformer-based feature matching (<a href="http://zju3dv.github.io/loftr">LoFTR</a>) and human reconstruction (<a href="http://zju3dv.github.io/neuralbody">NeuralBody</a> and <a href="http://zju3dv.github.io/Mirrored-Human">Mirrored-Human</a>) in CVPR 2021.
          </p>
      </div>
    </div>
  </div>
  -->


  <footer class="text-center" style="margin-bottom:10px; font-size: medium;">
      <hr>
      Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the <a href="https://lioryariv.github.io/idr/" target="_blank">website template</a>.
  </footer>

  <script type="text/javascript">
    function changePlaybackSpeed(speed)
        {
            document.getElementById('inspect_vid').playbackRate = speed;
        }
        // changePlaybackSpeed(0.25)

    var demo = document.getElementById("header_vid");
    var startTime;
    var timeout = undefined;
    demo.addEventListener("loadstart", function() {
      startTime = Date.now();
      timeout = setTimeout(function () {
        var demoWarning = document.getElementById("demo-warning");
        var giteeLink = document.createElement("a");
        giteeLink.innerText = "mirror hosted in mainland China";
        giteeLink.href = "https://project-pages-1255496016.cos-website.ap-shanghai.myqcloud.com/neuralrecon/";
        // var bilibiliLink = document.createElement("a");
        // var youtubeLink = document.createElement("a");
        // bilibiliLink.innerText = "BiliBili";
        // bilibiliLink.href = "";
        // youtubeLink.innerText = "YouTube";
        // youtubeLink.href = "";

        demoWarning.append("Loading the videos took too long, you can optionally visit this site in the ", giteeLink, ".");
        // demoWarning.append("Loading the video took too long, you can optionally watch it on Bilibili", bilibiliLink, " or YouTube", youtubeLink, ".");
        clearTimeout(timeout);
        timeout = undefined;
      }, 6000);
    });
    demo.addEventListener("loadeddata", function() {
      if (timeout) {
        clearTimeout(timeout);
        timeout = undefined;
      }
    });
//     var source = document.createElement("source");
//     source.setAttribute("src", "/videos/web-scene2.m4v");
//     source.setAttribute("type", "video/webm");
//     demo.appendChild(source);
  </script>
  <script>
    MathJax = {
      tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</body>
</html>
